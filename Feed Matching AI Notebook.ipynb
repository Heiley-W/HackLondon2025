{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🛠️ Step-by-Step Explanation: AI-Powered Feed Matching System\n",
    "This notebook provides a detailed breakdown of the **LDA-based topic modeling** and **Word2Vec-powered feed matching** algorithm.\n",
    "\n",
    "**Key Features:**\n",
    "- 🧠 **LDA (Latent Dirichlet Allocation)** for automatic topic classification.\n",
    "- 🔍 **Word2Vec embeddings** for semantic similarity measurement.\n",
    "- ⏳ **Recency Weighting** to prioritize fresh content.\n",
    "- 🤝 **Best Matching Feed Selection** using similarity + recency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔹 Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/heiley/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔹 Step 2: Define Sample User Feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feeds = {\n",
    "    \"Alice\": [\n",
    "        {\"text\": \"Bill Clinton fired 377,000 federal employees.\", \"timestamp\": datetime.now() - timedelta(days=0.4)},\n",
    "        {\"text\": \"Transportation is changing rapidly after years of slow progress.\", \"timestamp\": datetime.now() - timedelta(days=2)},\n",
    "        {\"text\": \"AI can help improve medical diagnosis through advanced image analysis.\", \"timestamp\": datetime.now() - timedelta(hours=12)},\n",
    "        {\"text\": \"Think Trump was aggressive to Zelenskyy? Watch the full talk.\", \"timestamp\": datetime.now() - timedelta(hours=6)},\n",
    "    ],\n",
    "    \"Bob\": [\n",
    "        {\"text\": \"I enjoy learning about political decisions and their impact.\", \"timestamp\": datetime.now() - timedelta(days=1)},\n",
    "        {\"text\": \"I find machine learning and its medical applications fascinating.\", \"timestamp\": datetime.now() - timedelta(hours=5)},\n",
    "        {\"text\": \"Exploring transportation advancements is an exciting topic for me.\", \"timestamp\": datetime.now() - timedelta(days=3)},\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔹 Step 3: LDA Topic Modeling for Automatic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text data for LDA topic modeling\n",
    "all_posts = [post[\"text\"] for posts in feeds.values() for post in posts]\n",
    "\n",
    "# Convert text into a document-term matrix using CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(all_posts)\n",
    "\n",
    "# Apply LDA for topic modeling with 3 topics\n",
    "lda_model = LatentDirichletAllocation(n_components=3, random_state=42)\n",
    "lda_model.fit(X)\n",
    "\n",
    "# Function to predict the most relevant topic for a given post\n",
    "def get_topic(text):\n",
    "    text_vec = vectorizer.transform([text])\n",
    "    topic_distribution = lda_model.transform(text_vec)\n",
    "    return np.argmax(topic_distribution)  # Returns topic index\n",
    "\n",
    "# Assign topics to posts\n",
    "topic_names = {0: \"Technology\", 1: \"Politics\", 2: \"AI\"}\n",
    "for user, posts in feeds.items():\n",
    "    for post in posts:\n",
    "        predicted_topic_index = get_topic(post[\"text\"])\n",
    "        post[\"topic\"] = topic_names[predicted_topic_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔹 Step 4: Word2Vec for Semantic Similarity Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text for Word2Vec training\n",
    "all_texts = [post[\"text\"] for posts in feeds.values() for post in posts]\n",
    "tokenized_sentences = [word_tokenize(text.lower()) for text in all_texts]\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to get embeddings\n",
    "def get_embedding(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    word_vectors = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
    "    return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(word2vec_model.vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔹 Step 5: Computing the Best Matching Feed Using Similarity & Recency Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate recency weight\n",
    "def recency_weight(timestamp, decay_rate=0.1):\n",
    "    days_since_post = (datetime.now() - timestamp).days + 1\n",
    "    return np.exp(-decay_rate * days_since_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LDA Topic                          Alice's Best Matched Post  \\\n",
      "0  Politics  Think Trump was aggressive to Zelenskyy? Watch...   \n",
      "\n",
      "                             Bob's Best Matched Post  \\\n",
      "0  Exploring transportation advancements is an ex...   \n",
      "\n",
      "   Highest Similarity Score  Final Weighted Score  \n",
      "0                      0.27                  0.21  \n"
     ]
    }
   ],
   "source": [
    "# Compute the best matching feed between Alice and Bob\n",
    "best_match = {}\n",
    "max_weighted_score = 0\n",
    "\n",
    "# Group posts by predicted LDA topic\n",
    "topic_groups = defaultdict(list)\n",
    "for user, posts in feeds.items():\n",
    "    for post in posts:\n",
    "        topic_groups[post[\"topic\"]].append(post)\n",
    "\n",
    "# Find best match based on LDA topics, similarity, and recency\n",
    "for topic, posts in topic_groups.items():\n",
    "    user_posts = {\"Alice\": [], \"Bob\": []}\n",
    "\n",
    "    # Separate posts by user\n",
    "    for post in posts:\n",
    "        if post in feeds[\"Alice\"]:\n",
    "            user_posts[\"Alice\"].append(post)\n",
    "        elif post in feeds[\"Bob\"]:\n",
    "            user_posts[\"Bob\"].append(post)\n",
    "\n",
    "    # Compare only within the same LDA topic\n",
    "    for alice_post in user_posts[\"Alice\"]:\n",
    "        for bob_post in user_posts[\"Bob\"]:\n",
    "            alice_embedding = get_embedding(alice_post[\"text\"])\n",
    "            bob_embedding = get_embedding(bob_post[\"text\"])\n",
    "\n",
    "            similarity_score = cosine_similarity([alice_embedding], [bob_embedding])[0][0]\n",
    "            alice_weight = recency_weight(alice_post[\"timestamp\"])\n",
    "            bob_weight = recency_weight(bob_post[\"timestamp\"])\n",
    "\n",
    "            # Weighted score = similarity * average recency weight\n",
    "            weighted_score = similarity_score * ((alice_weight + bob_weight) / 2)\n",
    "\n",
    "            if weighted_score > max_weighted_score:\n",
    "                max_weighted_score = weighted_score\n",
    "                best_match = {\n",
    "                    \"LDA Topic\": topic,\n",
    "                    \"Alice's Best Matched Post\": alice_post[\"text\"],\n",
    "                    \"Bob's Best Matched Post\": bob_post[\"text\"],\n",
    "                    \"Highest Similarity Score\": round(similarity_score, 2),\n",
    "                    \"Final Weighted Score\": round(weighted_score, 2),\n",
    "                }\n",
    "\n",
    "# Convert results to DataFrame and display\n",
    "best_match_df = pd.DataFrame([best_match])\n",
    "print(best_match_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
